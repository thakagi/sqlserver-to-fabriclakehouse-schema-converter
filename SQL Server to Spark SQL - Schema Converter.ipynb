{"cells":[{"cell_type":"markdown","source":["## Convert SQL Server table definitions to Spark SQL table definitions for Fabric Lakehouse"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"dc5cfb33-0802-493a-94da-e409acd95a0c"},{"cell_type":"markdown","source":["### 1. Set the SQL Server DDL to be converted"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"3d84d008-7b47-4e55-a88b-14b55e0c3a6a"},{"cell_type":"code","source":["# SQL Server DDL Statements (Multiple Tables Supported)\n","# Paste the target DDL into sql_input\n","sql_input = \"\"\"\n","CREATE TABLE [dbo].[Sales] (\n","    [EnrollmentId] INT IDENTITY(1,1) PRIMARY KEY,\n","    [CustomerName] VARCHAR(100) NOT NULL,\n","    [Amount] DECIMAL(10,2),\n","    [CreatedDate] DATETIME,\n","    [IsActive] BIT,\n","    CONSTRAINT PK_Sales PRIMARY KEY (EnrollmentId)\n",");\n","\n","CREATE TABLE [dbo2].[Products] (\n","    [ProductId] INT IDENTITY(1,1),\n","    [ProductName] VARCHAR(255) NOT NULL,\n","    [Price] DECIMAL(8,2),\n","    [CreatedOn] DATETIME DEFAULT GETDATE()\n",");\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"46f6661f-8caa-4cc8-ac1e-6fb7f38f5fda"},{"cell_type":"markdown","source":["### 2. Data Type Mapping"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffb5eff2-48fe-42db-b036-0ff4251aa553"},{"cell_type":"code","source":["# SQL Server -> Spark SQL type mapping definition\n","# Edit as needed\n","SQLSERVER_TO_SPARK_TYPE_MAP = {\n","    # Numeric types\n","    \"TINYINT\": \"INT\",\n","    \"SMALLINT\": \"INT\",\n","    \"INT\": \"INT\",\n","    \"BIGINT\": \"BIGINT\",\n","    \"DECIMAL\": \"DECIMAL\",       # Precision will be added later\n","    \"NUMERIC\": \"DECIMAL\",       # Same as above\n","    \"FLOAT\": \"DOUBLE\",\n","    \"REAL\": \"FLOAT\",            # Change it to DOUBLE if you value accuracy\n","\n","    # Currency type\n","    \"MONEY\": \"DECIMAL(19,4)\",\n","    \"SMALLMONEY\": \"DECIMAL(10,4)\",\n","\n","    # String type\n","    \"CHAR\": \"STRING\",\n","    \"NCHAR\": \"STRING\",\n","    \"VARCHAR\": \"STRING\",\n","    \"NVARCHAR\": \"STRING\",\n","    \"TEXT\": \"STRING\",\n","    \"NTEXT\": \"STRING\",\n","\n","    # Date/Time Types\n","    \"DATE\": \"DATE\",\n","    \"TIME\": \"STRING\",              # Spark does not have a TIME type\n","    \"DATETIME\": \"TIMESTAMP\",\n","    \"DATETIME2\": \"TIMESTAMP\",\n","    \"SMALLDATETIME\": \"TIMESTAMP\",\n","    \"DATETIMEOFFSET\": \"TIMESTAMP\",  # Timezone is ignored\n","\n","    # Logical types\n","    \"BIT\": \"BOOLEAN\",\n","\n","    # Binary types\n","    \"BINARY\": \"BINARY\",\n","    \"VARBINARY\": \"BINARY\",\n","    \"IMAGE\": \"BINARY\",\n","\n","    # Other special types\n","    \"UNIQUEIDENTIFIER\": \"STRING\",     # Can be used as UUID\n","    \"XML\": \"STRING\",\n","    \"SQL_VARIANT\": \"STRING\",          # Substituted with string\n","    #\"CURSOR\": \"STRING\",               # Not available. It is recommended to issue a warning\n","    #\"TABLE\": \"STRING\",                # Not available. It is recommended to issue a warning\n","    \"HIERARCHYID\": \"STRING\",          # Special type. Usually held by ID\n","    \"GEOGRAPHY\": \"STRING\",            # Spatial data, sometimes stored in WKT format\n","    \"GEOMETRY\": \"STRING\",\n","    \"ROWVERSION\": \"BINARY\",           # Alternatively can convert it to BIGINT\n","    \"TIMESTAMP\": \"BINARY\"             # SQL Server Timestamp is for version\n","}\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c98d6dc3-e474-4596-9aa3-cf6c6b50e5df"},{"cell_type":"markdown","source":["### 3. Check data type mapping"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91304646-d359-4dd8-a080-3e3fd076613b"},{"cell_type":"code","source":["def detect_unmapped_types(sql_text: str, type_map: dict) -> set:\n","    # Remove comments\n","    sql_text = re.sub(r'--.*', '', sql_text)\n","    sql_text = re.sub(r'/\\*.*?\\*/', '', sql_text, flags=re.DOTALL)\n","\n","    # Multiple CREATE TABLE support\n","    create_blocks = re.findall(r'CREATE\\s+TABLE.*?\\((.*?)\\);', sql_text, flags=re.IGNORECASE | re.DOTALL)\n","    used_types = set()\n","\n","    for block in create_blocks:\n","        lines = block.splitlines()\n","        for line in lines:\n","            line = line.strip().rstrip(',')\n","\n","            # Exclude table constraints\n","            if not line or re.match(r'CONSTRAINT|PRIMARY\\s+KEY|FOREIGN\\s+KEY', line, flags=re.IGNORECASE):\n","                continue\n","\n","            # Extract column name + data type\n","            match = re.match(r'\\[?\\w+\\]?\\s+([a-zA-Z0-9_]+)', line)\n","            if match:\n","                dtype = match.group(1).upper()\n","                used_types.add(dtype)\n","\n","    # Extract Unmapped Types\n","    unmapped = {t for t in used_types if t not in type_map}\n","    return unmapped\n","\n","\n","# Check if unmapped types are included\n","unmapped = detect_unmapped_types(sql_input, SQLSERVER_TO_SPARK_TYPE_MAP)\n","if unmapped:\n","    print(\"Unmapped SQL Server data types found:\")\n","    for dtype in unmapped:\n","        print(f\"  - {dtype}\")\n","else:\n","    print(\"All data types are mapped.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e362e893-ac0a-4988-8b4d-7bec023576aa"},{"cell_type":"markdown","source":["### 4. Define the conversion function"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"932ddb20-c2aa-40a5-83d8-1c147c8293c8"},{"cell_type":"code","source":["import re\n","\n","def remove_comments(sql_text):\n","    sql_text = re.sub(r'--.*', '', sql_text)\n","    sql_text = re.sub(r'/\\*.*?\\*/', '', sql_text, flags=re.DOTALL)\n","    return sql_text\n","\n","def extract_create_statements(sql_text):\n","    return re.findall(r'CREATE\\s+TABLE\\s+.*?\\(.*?\\);', sql_text, re.IGNORECASE | re.DOTALL)\n","\n","def clean_column_line(line, type_map):\n","    line = line.strip().rstrip(',')\n","    if not line or line.upper().startswith(\"CONSTRAINT\") or \"PRIMARY KEY\" in line.upper():\n","        return None\n","\n","    line = re.sub(r'\\bIDENTITY\\s*\\(\\d+,\\s*\\d+\\)', '', line, flags=re.IGNORECASE)\n","    line = re.sub(r'\\bDEFAULT\\b\\s+[^\\s,]+', '', line, flags=re.IGNORECASE)\n","    line = re.sub(r'\\bNOT\\s+NULL\\b|\\bNULL\\b', '', line, flags=re.IGNORECASE)\n","\n","    match = re.match(r'\\[?(\\w+)\\]?\\s+([A-Z]+)(\\(\\d+(,\\d+)?\\))?', line, re.IGNORECASE)\n","    if not match:\n","        return None\n","\n","    col_name = match.group(1)\n","    col_type = match.group(2).upper()\n","    precision = match.group(3) or \"\"\n","\n","    spark_type = type_map.get(col_type, \"STRING\")\n","    if spark_type == \"DECIMAL\" and precision:\n","        spark_type += precision\n","\n","    return f\"  {col_name} {spark_type}\"\n","\n","def convert_sqlserver_to_spark(sql_text: str, type_map: dict, table_format=\"delta\", remove_schema=False) -> list:\n","    sql_text = remove_comments(sql_text)\n","    create_statements = extract_create_statements(sql_text)\n","    results = []\n","\n","    for stmt in create_statements:\n","        table_match = re.search(r'CREATE\\s+TABLE\\s+(\\[?\\w+\\]?\\.?\\[?\\w+\\]?)', stmt, re.IGNORECASE)\n","        full_table_name = table_match.group(1).replace('[', '').replace(']', '') if table_match else \"converted_table\"\n","\n","        if remove_schema and '.' in full_table_name:\n","            table_name = full_table_name.split('.')[-1]\n","        else:\n","            table_name = full_table_name\n","\n","        inner = re.search(r'\\((.*)\\)', stmt, re.DOTALL)\n","        if not inner:\n","            continue\n","\n","        column_lines = inner.group(1).splitlines()\n","        spark_columns = []\n","\n","        for line in column_lines:\n","            converted = clean_column_line(line, type_map)\n","            if converted:\n","                spark_columns.append(converted)\n","\n","        spark_ddl = f\"CREATE TABLE {table_name} (\\n\" + \",\\n\".join(spark_columns) + f\"\\n) USING {table_format};\"\n","        results.append((table_name, spark_ddl))\n","\n","    return results"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ffa0d6c-75f6-458c-8fb7-8fd25cf42fae"},{"cell_type":"markdown","source":["### 5. Create table with converted DDL"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d0bdb601-0abd-42dd-9639-17f2876e84f6"},{"cell_type":"code","source":["# If you want to run the DDL with Spark SQL, uncomment the last line.\n","# If you are using Lakehouse with a schema, change the argument to \"remove_schema=False\".\n","converted = convert_sqlserver_to_spark(sql_input, type_map=SQLSERVER_TO_SPARK_TYPE_MAP, table_format=\"delta\", remove_schema=True)\n","\n","for table_name, spark_sql in converted:\n","    print(f\"\\n--- Creating table: {table_name} ---\\n\")\n","    print(spark_sql)\n","    #spark.sql(spark_sql)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b5b54bb7-ed49-4497-905a-f2f5f02be38c"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","language":null,"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":null}},"nbformat":4,"nbformat_minor":5}